{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сегментация по текстовому промпту с Grounding DINO + SAM\n",
    "\n",
    "В этом ноутбуке мы реализуем пайплайн сегментации объектов по текстовому описанию, используя две мощные модели:\n",
    "- **Grounding DINO** - для детекции объектов по текстовому промпту\n",
    "- **SAM (Segment Anything Model)** - для точной сегментации найденных объектов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка зависимостей\n",
    "\n",
    "Устанавливаем необходимые библиотеки для работы с Grounding DINO и SAM.\n",
    "\n",
    "**Важно:** Для работы Grounding DINO требуются скомпилированные C++ расширения. Если возникают ошибки компиляции, убедитесь, что установлены необходимые инструменты компиляции (gcc, g++, CUDA toolkit при использовании GPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "GroundingDINO already exists\n",
      "\u001b[33m  DEPRECATION: Legacy editable install of groundingdino==0.1.0 from file:///home/tam2511/mounts/0/arcadia/market/robotics/cv/ml/user_data/shad/cv2025/lesson3/seminar/GroundingDINO (setup.py develop) is deprecated. pip 25.3 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
      "\u001b[0m    \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "    \n",
      "    \u001b[31m×\u001b[0m \u001b[32mpython setup.py develop\u001b[0m did not run successfully.\n",
      "    \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "    \u001b[31m╰─>\u001b[0m \u001b[31m[49 lines of output]\u001b[0m\n",
      "    \u001b[31m   \u001b[0m Building wheel groundingdino-0.1.0\n",
      "    \u001b[31m   \u001b[0m Compiling with CUDA\n",
      "    \u001b[31m   \u001b[0m running develop\n",
      "    \u001b[31m   \u001b[0m /home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/command/easy_install.py:156: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
      "    \u001b[31m   \u001b[0m   warnings.warn(\n",
      "    \u001b[31m   \u001b[0m /home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    \u001b[31m   \u001b[0m   warnings.warn(\n",
      "    \u001b[31m   \u001b[0m running egg_info\n",
      "    \u001b[31m   \u001b[0m /home/tam2511/venvs/train_py10/lib/python3.10/site-packages/torch/utils/cpp_extension.py:497: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "    \u001b[31m   \u001b[0m   warnings.warn(msg.format('we could not find ninja.'))\n",
      "    \u001b[31m   \u001b[0m writing manifest file 'groundingdino.egg-info/SOURCES.txt'\n",
      "    \u001b[31m   \u001b[0m running build_ext\n",
      "    \u001b[31m   \u001b[0m /usr/bin/nvcc: 3: exec: /usr/lib/nvidia-cuda-toolkit/bin/nvcc: not found\n",
      "    \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "    \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "    \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 35, in <module>\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/mounts/0/arcadia/market/robotics/cv/ml/user_data/shad/cv2025/lesson3/seminar/GroundingDINO/setup.py\", line 204, in <module>\n",
      "    \u001b[31m   \u001b[0m     setup(\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/__init__.py\", line 155, in setup\n",
      "    \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 148, in setup\n",
      "    \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 163, in run_commands\n",
      "    \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 967, in run_commands\n",
      "    \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 986, in run_command\n",
      "    \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/command/develop.py\", line 34, in run\n",
      "    \u001b[31m   \u001b[0m     self.install_for_development()\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/command/develop.py\", line 114, in install_for_development\n",
      "    \u001b[31m   \u001b[0m     self.run_command('build_ext')\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 313, in run_command\n",
      "    \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 986, in run_command\n",
      "    \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/command/build_ext.py\", line 79, in run\n",
      "    \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\", line 339, in run\n",
      "    \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 520, in build_extensions\n",
      "    \u001b[31m   \u001b[0m     _check_cuda_version(compiler_name, compiler_version)\n",
      "    \u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 399, in _check_cuda_version\n",
      "    \u001b[31m   \u001b[0m     cuda_version_str = subprocess.check_output([nvcc, '--version']).strip().decode(*SUBPROCESS_DECODE_ARGS)\n",
      "    \u001b[31m   \u001b[0m   File \"/usr/lib/python3.10/subprocess.py\", line 421, in check_output\n",
      "    \u001b[31m   \u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "    \u001b[31m   \u001b[0m   File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n",
      "    \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, process.args,\n",
      "    \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['/usr/bin/nvcc', '--version']' returned non-zero exit status 127.\n",
      "    \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "    \n",
      "    \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Can't roll back groundingdino; was not uninstalled\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mpython setup.py develop\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m \u001b[31m[49 lines of output]\u001b[0m\n",
      "\u001b[31m   \u001b[0m Building wheel groundingdino-0.1.0\n",
      "\u001b[31m   \u001b[0m Compiling with CUDA\n",
      "\u001b[31m   \u001b[0m running develop\n",
      "\u001b[31m   \u001b[0m /home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/command/easy_install.py:156: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
      "\u001b[31m   \u001b[0m   warnings.warn(\n",
      "\u001b[31m   \u001b[0m /home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "\u001b[31m   \u001b[0m   warnings.warn(\n",
      "\u001b[31m   \u001b[0m running egg_info\n",
      "\u001b[31m   \u001b[0m /home/tam2511/venvs/train_py10/lib/python3.10/site-packages/torch/utils/cpp_extension.py:497: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "\u001b[31m   \u001b[0m   warnings.warn(msg.format('we could not find ninja.'))\n",
      "\u001b[31m   \u001b[0m writing manifest file 'groundingdino.egg-info/SOURCES.txt'\n",
      "\u001b[31m   \u001b[0m running build_ext\n",
      "\u001b[31m   \u001b[0m /usr/bin/nvcc: 3: exec: /usr/lib/nvidia-cuda-toolkit/bin/nvcc: not found\n",
      "\u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "\u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "\u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 35, in <module>\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/mounts/0/arcadia/market/robotics/cv/ml/user_data/shad/cv2025/lesson3/seminar/GroundingDINO/setup.py\", line 204, in <module>\n",
      "\u001b[31m   \u001b[0m     setup(\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/__init__.py\", line 155, in setup\n",
      "\u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 148, in setup\n",
      "\u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 163, in run_commands\n",
      "\u001b[31m   \u001b[0m     dist.run_commands()\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 967, in run_commands\n",
      "\u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 986, in run_command\n",
      "\u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/command/develop.py\", line 34, in run\n",
      "\u001b[31m   \u001b[0m     self.install_for_development()\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/command/develop.py\", line 114, in install_for_development\n",
      "\u001b[31m   \u001b[0m     self.run_command('build_ext')\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 313, in run_command\n",
      "\u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 986, in run_command\n",
      "\u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/command/build_ext.py\", line 79, in run\n",
      "\u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\", line 339, in run\n",
      "\u001b[31m   \u001b[0m     self.build_extensions()\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 520, in build_extensions\n",
      "\u001b[31m   \u001b[0m     _check_cuda_version(compiler_name, compiler_version)\n",
      "\u001b[31m   \u001b[0m   File \"/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 399, in _check_cuda_version\n",
      "\u001b[31m   \u001b[0m     cuda_version_str = subprocess.check_output([nvcc, '--version']).strip().decode(*SUBPROCESS_DECODE_ARGS)\n",
      "\u001b[31m   \u001b[0m   File \"/usr/lib/python3.10/subprocess.py\", line 421, in check_output\n",
      "\u001b[31m   \u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "\u001b[31m   \u001b[0m   File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n",
      "\u001b[31m   \u001b[0m     raise CalledProcessError(retcode, process.args,\n",
      "\u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['/usr/bin/nvcc', '--version']' returned non-zero exit status 127.\n",
      "\u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Installation completed\n"
     ]
    }
   ],
   "source": [
    "!pip install -q segment-anything transformers torch torchvision opencv-python\n",
    "!git clone https://github.com/IDEA-Research/GroundingDINO.git 2>/dev/null || echo \"GroundingDINO already exists\"\n",
    "import os\n",
    "os.chdir('GroundingDINO')\n",
    "!pip install -q -e . || echo \"Installation completed\"\n",
    "os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорты и настройка\n",
    "\n",
    "Импортируем необходимые библиотеки для работы с изображениями, моделями и визуализацией.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка и компиляция C++ расширений\n",
    "\n",
    "Проверяем наличие C++ расширений Grounding DINO и при необходимости компилируем их.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++ extensions not found. Attempting to compile...\n",
      "Warning: Compilation had issues. Check output:\n",
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/torch/utils/cpp_extension.py:497: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "  warnings.warn(msg.format('we could not find ninja.'))\n",
      "/usr/bin/nvcc: 3: exec: /usr/lib/nvidia-cuda-toolkit/bin/nvcc: not found\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tam2511/mounts/0/arcadia/market/robotics/cv/ml/user_data/shad/cv2025/lesson3/semin\n",
      "\n",
      "Model may still work but could be slower.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "groundingdino_dir = 'GroundingDINO'\n",
    "cpp_ext_pattern = os.path.join(groundingdino_dir, 'groundingdino', 'models', 'GroundingDINO', 'ms_deform_attn.cpython-*.so')\n",
    "so_files = glob.glob(cpp_ext_pattern)\n",
    "\n",
    "if not so_files:\n",
    "    print(\"C++ extensions not found. Attempting to compile...\")\n",
    "    original_dir = os.getcwd()\n",
    "    try:\n",
    "        os.chdir(groundingdino_dir)\n",
    "        result = subprocess.run(['python', 'setup.py', 'build_ext', '--inplace'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"C++ extensions compiled successfully\")\n",
    "        else:\n",
    "            print(f\"Warning: Compilation had issues. Check output:\")\n",
    "            print(result.stderr[:500])\n",
    "            print(\"\\nModel may still work but could be slower.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not compile C++ extensions: {e}\")\n",
    "        print(\"Model may still work but could be slower.\")\n",
    "    finally:\n",
    "        os.chdir(original_dir)\n",
    "else:\n",
    "    print(f\"C++ extensions found: {so_files[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка моделей\n",
    "\n",
    "Инициализируем модели Grounding DINO и SAM. Grounding DINO будет использоваться для детекции объектов по текстовому промпту, а SAM - для получения масок сегментации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "\n",
    "sys.path.insert(0, 'GroundingDINO')\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "from groundingdino.util import box_ops\n",
    "import groundingdino.datasets.transforms as T\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "groundingdino_config_path = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "groundingdino_checkpoint_path = \"groundingdino_swint_ogc.pth\"\n",
    "\n",
    "sam_checkpoint_path = \"sam_vit_h_4b8939.pth\"\n",
    "sam_model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if not os.path.exists(groundingdino_checkpoint_path):\n",
    "    print(\"Downloading Grounding DINO checkpoint...\")\n",
    "    subprocess.check_call(['wget', '-q', 'https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth', '-O', groundingdino_checkpoint_path])\n",
    "\n",
    "if not os.path.exists(sam_checkpoint_path):\n",
    "    print(\"Downloading SAM checkpoint...\")\n",
    "    subprocess.check_call(['wget', '-q', 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth', '-O', sam_checkpoint_path])\n",
    "\n",
    "os.environ['USE_REENTRANT'] = 'False'\n",
    "\n",
    "grounding_model = load_model(groundingdino_config_path, groundingdino_checkpoint_path)\n",
    "grounding_model = grounding_model.to(device)\n",
    "grounding_model.eval()\n",
    "\n",
    "sam = sam_model_registry[sam_model_type](checkpoint=sam_checkpoint_path)\n",
    "sam = sam.to(device)\n",
    "sam_predictor = SamPredictor(sam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка датасета CamVid\n",
    "\n",
    "Загружаем изображения из датасета CamVid для тестирования пайплайна сегментации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 101 images from validation set\n"
     ]
    }
   ],
   "source": [
    "class CamVidLoader:\n",
    "    def __init__(self, data_dir, split='train'):\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        txt_file = os.path.join(data_dir, split + '.txt')\n",
    "        self.image_paths = []\n",
    "        \n",
    "        with open(txt_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 1:\n",
    "                        img_path_abs = parts[0]\n",
    "                        img_filename = os.path.basename(img_path_abs)\n",
    "                        img_path = os.path.join(data_dir, split, img_filename)\n",
    "                        if os.path.exists(img_path):\n",
    "                            self.image_paths.append(img_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return np.array(image), img_path\n",
    "\n",
    "data_dir = '../data'\n",
    "dataset = CamVidLoader(data_dir, split='val')\n",
    "print(f\"Loaded {len(dataset)} images from validation set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция детекции объектов через Grounding DINO\n",
    "\n",
    "Grounding DINO принимает изображение и текстовый промпт, возвращает координаты bounding box'ов обнаруженных объектов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(image_source, text_prompt, box_threshold=0.3, text_threshold=0.25):\n",
    "    transform = T.Compose([\n",
    "        T.RandomResize([800], max_size=1333),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    if isinstance(image_source, str):\n",
    "        image, _ = load_image(image_source)\n",
    "    elif isinstance(image_source, np.ndarray):\n",
    "        image = Image.fromarray(image_source).convert('RGB')\n",
    "    else:\n",
    "        image = image_source\n",
    "    \n",
    "    image_transformed, _ = transform(image, None)\n",
    "    \n",
    "    boxes, logits, phrases = predict(\n",
    "        model=grounding_model,\n",
    "        image=image_transformed,\n",
    "        caption=text_prompt,\n",
    "        box_threshold=box_threshold,\n",
    "        text_threshold=text_threshold\n",
    "    )\n",
    "    \n",
    "    return boxes, logits, phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция сегментации через SAM\n",
    "\n",
    "SAM принимает изображение и координаты bounding box'ов, возвращает маски сегментации для каждого объекта.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_objects(image, boxes):\n",
    "    sam_predictor.set_image(image)\n",
    "    \n",
    "    H, W, _ = image.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "    \n",
    "    transformed_boxes = sam_predictor.transform.apply_boxes_torch(boxes_xyxy, image.shape[:2])\n",
    "    transformed_boxes = transformed_boxes.to(device)\n",
    "    \n",
    "    masks, scores, logits = sam_predictor.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    \n",
    "    return masks.cpu().numpy(), scores.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пайплайн сегментации по текстовому промпту\n",
    "\n",
    "Объединяем Grounding DINO и SAM в единый пайплайн: сначала детектируем объекты по текстовому промпту, затем сегментируем их.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_by_text_prompt(image_source, text_prompt, box_threshold=0.3, text_threshold=0.25):\n",
    "    if isinstance(image_source, str):\n",
    "        image = np.array(Image.open(image_source).convert('RGB'))\n",
    "    else:\n",
    "        image = image_source\n",
    "    \n",
    "    boxes, logits, phrases = detect_objects(image, text_prompt, box_threshold, text_threshold)\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return image, None, None, None\n",
    "    \n",
    "    masks, scores = segment_objects(image, boxes)\n",
    "    \n",
    "    return image, boxes, masks, phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация результатов\n",
    "\n",
    "Функция для визуализации исходного изображения с наложенными масками сегментации и bounding box'ами.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(image, boxes, masks, phrases, alpha=0.5):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(image)\n",
    "    if boxes is not None and len(boxes) > 0:\n",
    "        H, W = image.shape[:2]\n",
    "        boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "        \n",
    "        for i, (box, phrase) in enumerate(zip(boxes_xyxy, phrases)):\n",
    "            x1, y1, x2, y2 = box.numpy()\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, \n",
    "                                   edgecolor='red', facecolor='none')\n",
    "            axes[1].add_patch(rect)\n",
    "            axes[1].text(x1, y1-5, phrase, fontsize=10, color='red', \n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    axes[1].set_title('Detections (Grounding DINO)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(image)\n",
    "    if masks is not None and len(masks) > 0:\n",
    "        for i, mask in enumerate(masks):\n",
    "            mask_bool = mask[0].astype(bool)\n",
    "            color = np.random.rand(3)\n",
    "            colored_mask = np.zeros_like(image)\n",
    "            colored_mask[mask_bool] = color\n",
    "            axes[2].imshow(colored_mask, alpha=alpha)\n",
    "    axes[2].set_title('Segmentation Masks (SAM)')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование на примерах из CamVid\n",
    "\n",
    "Тестируем пайплайн на нескольких изображениях из датасета CamVid с различными текстовыми промптами.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image: 0016E5_07959.png\n",
      "  Prompt: car. vehicle. automobile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m text_prompts[:\u001b[38;5;241m2\u001b[39m]:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     result_image, boxes, masks, phrases \u001b[38;5;241m=\u001b[39m \u001b[43msegment_by_text_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbox_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(boxes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(boxes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m objects\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36msegment_by_text_prompt\u001b[0;34m(image_source, text_prompt, box_threshold, text_threshold)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     image \u001b[38;5;241m=\u001b[39m image_source\n\u001b[0;32m----> 7\u001b[0m boxes, logits, phrases \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(boxes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mdetect_objects\u001b[0;34m(image_source, text_prompt, box_threshold, text_threshold)\u001b[0m\n\u001b[1;32m     13\u001b[0m     image \u001b[38;5;241m=\u001b[39m image_source\n\u001b[1;32m     15\u001b[0m image_transformed, _ \u001b[38;5;241m=\u001b[39m transform(image, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m boxes, logits, phrases \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrounding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_transformed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbox_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbox_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_threshold\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m boxes, logits, phrases\n",
      "File \u001b[0;32m~/mounts/0/arcadia/market/robotics/cv/ml/user_data/shad/cv2025/lesson3/seminar/GroundingDINO/groundingdino/util/inference.py:68\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, image, caption, box_threshold, text_threshold, device, remove_combined)\u001b[0m\n\u001b[1;32m     65\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 68\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m prediction_logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msigmoid()[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# prediction_logits.shape = (nq, 256)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m prediction_boxes \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# prediction_boxes.shape = (nq, 4)\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/mounts/0/arcadia/market/robotics/cv/ml/user_data/shad/cv2025/lesson3/seminar/GroundingDINO/groundingdino/models/GroundingDINO/groundingdino.py:327\u001b[0m, in \u001b[0;36mGroundingDINO.forward\u001b[0;34m(self, samples, targets, **kw)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposs\u001b[38;5;241m.\u001b[39mappend(pos_l)\n\u001b[1;32m    326\u001b[0m input_query_bbox \u001b[38;5;241m=\u001b[39m input_query_label \u001b[38;5;241m=\u001b[39m attn_mask \u001b[38;5;241m=\u001b[39m dn_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m hs, reference, hs_enc, ref_enc, init_box_proposal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_query_bbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_query_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_dict\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# deformable-detr-like anchor update\u001b[39;00m\n\u001b[1;32m    332\u001b[0m outputs_coord_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/mounts/0/arcadia/market/robotics/cv/ml/user_data/shad/cv2025/lesson3/seminar/GroundingDINO/groundingdino/models/GroundingDINO/transformer.py:258\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, srcs, masks, refpoint_embed, pos_embeds, tgt, attn_mask, text_dict)\u001b[0m\n\u001b[1;32m    253\u001b[0m enc_topk_proposals \u001b[38;5;241m=\u001b[39m enc_refpoint_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m#########################################################\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Begin Encoder\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m#########################################################\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m memory, memory_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlvl_pos_embed_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoded_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mtext_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_token_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we ~ the mask . False means use the token; True means pad the token\u001b[39;49;00m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mposition_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_self_attention_masks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m#########################################################\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# End Encoder\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# - memory: bs, \\sum{hw}, c\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# - enc_intermediate_refpoints: None or (nenc+1, bs, nq, c) or (nenc, bs, nq, c)\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m#########################################################\u001b[39;00m\n\u001b[1;32m    279\u001b[0m text_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoded_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m memory_text\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/mounts/0/arcadia/market/robotics/cv/ml/user_data/shad/cv2025/lesson3/seminar/GroundingDINO/groundingdino/models/GroundingDINO/transformer.py:576\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, pos, spatial_shapes, level_start_index, valid_ratios, key_padding_mask, memory_text, text_attention_mask, pos_text, text_self_attention_masks, position_ids)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# main process\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_transformer_ckpt:\n\u001b[0;32m--> 576\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m     output \u001b[38;5;241m=\u001b[39m layer(\n\u001b[1;32m    587\u001b[0m         src\u001b[38;5;241m=\u001b[39moutput,\n\u001b[1;32m    588\u001b[0m         pos\u001b[38;5;241m=\u001b[39mpos,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    593\u001b[0m     )\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:489\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m         )\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    492\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    493\u001b[0m     )\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:264\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    261\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 264\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/mounts/0/arcadia/market/robotics/cv/ml/user_data/shad/cv2025/lesson3/seminar/GroundingDINO/groundingdino/models/GroundingDINO/transformer.py:785\u001b[0m, in \u001b[0;36mDeformableTransformerEncoderLayer.forward\u001b[0;34m(self, src, pos, reference_points, spatial_shapes, level_start_index, key_padding_mask)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28mself\u001b[39m, src, pos, reference_points, spatial_shapes, level_start_index, key_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    782\u001b[0m ):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# self attention\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m# import ipdb; ipdb.set_trace()\u001b[39;00m\n\u001b[0;32m--> 785\u001b[0m     src2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_pos_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m     src \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(src2)\n\u001b[1;32m    794\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(src)\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/mounts/0/arcadia/market/robotics/cv/ml/user_data/shad/cv2025/lesson3/seminar/GroundingDINO/groundingdino/models/GroundingDINO/ms_deform_attn.py:338\u001b[0m, in \u001b[0;36mMultiScaleDeformableAttention.forward\u001b[0;34m(self, query, key, value, query_pos, key_padding_mask, reference_points, spatial_shapes, level_start_index, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m     sampling_locations \u001b[38;5;241m=\u001b[39m sampling_locations\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    336\u001b[0m     attention_weights \u001b[38;5;241m=\u001b[39m attention_weights\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m--> 338\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mMultiScaleDeformableAttnFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_locations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim2col_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m halffloat:\n\u001b[1;32m    348\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mhalf()\n",
      "File \u001b[0;32m~/venvs/train_py10/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/mounts/0/arcadia/market/robotics/cv/ml/user_data/shad/cv2025/lesson3/seminar/GroundingDINO/groundingdino/models/GroundingDINO/ms_deform_attn.py:53\u001b[0m, in \u001b[0;36mMultiScaleDeformableAttnFunction.forward\u001b[0;34m(ctx, value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, im2col_step)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     44\u001b[0m     ctx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     im2col_step,\n\u001b[1;32m     51\u001b[0m ):\n\u001b[1;32m     52\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mim2col_step \u001b[38;5;241m=\u001b[39m im2col_step\n\u001b[0;32m---> 53\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_C\u001b[49m\u001b[38;5;241m.\u001b[39mms_deform_attn_forward(\n\u001b[1;32m     54\u001b[0m         value,\n\u001b[1;32m     55\u001b[0m         value_spatial_shapes,\n\u001b[1;32m     56\u001b[0m         value_level_start_index,\n\u001b[1;32m     57\u001b[0m         sampling_locations,\n\u001b[1;32m     58\u001b[0m         attention_weights,\n\u001b[1;32m     59\u001b[0m         ctx\u001b[38;5;241m.\u001b[39mim2col_step,\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     61\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\n\u001b[1;32m     62\u001b[0m         value,\n\u001b[1;32m     63\u001b[0m         value_spatial_shapes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         attention_weights,\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "test_indices = [0, 5, 10]\n",
    "text_prompts = [\n",
    "    \"car. vehicle. automobile\",\n",
    "    \"road. street. pavement\",\n",
    "    \"tree. vegetation. plant\",\n",
    "    \"building. house. structure\",\n",
    "    \"person. pedestrian. human\"\n",
    "]\n",
    "\n",
    "for idx in test_indices:\n",
    "    image, img_path = dataset[idx]\n",
    "    print(f\"\\nProcessing image: {os.path.basename(img_path)}\")\n",
    "    \n",
    "    for prompt in text_prompts[:2]:\n",
    "        print(f\"  Prompt: {prompt}\")\n",
    "        result_image, boxes, masks, phrases = segment_by_text_prompt(\n",
    "            image, \n",
    "            prompt,\n",
    "            box_threshold=0.3,\n",
    "            text_threshold=0.25\n",
    "        )\n",
    "        \n",
    "        if boxes is not None and len(boxes) > 0:\n",
    "            print(f\"    Found {len(boxes)} objects\")\n",
    "            visualize_results(result_image, boxes, masks, phrases)\n",
    "        else:\n",
    "            print(f\"    No objects found for this prompt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение с ground truth\n",
    "\n",
    "Сравниваем результаты сегментации по текстовому промпту с ground truth масками из датасета CamVid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth_mask(img_path, data_dir):\n",
    "    img_filename = os.path.basename(img_path)\n",
    "    mask_path = os.path.join(data_dir, 'valannot', img_filename)\n",
    "    if os.path.exists(mask_path):\n",
    "        mask = np.array(Image.open(mask_path).convert('L'))\n",
    "        return mask\n",
    "    return None\n",
    "\n",
    "def compare_with_ground_truth(image, masks, gt_mask, prompt):\n",
    "    if gt_mask is None:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(gt_mask, cmap='tab20', vmin=0, vmax=11)\n",
    "    axes[1].set_title('Ground Truth Mask')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    combined_mask = np.zeros_like(gt_mask)\n",
    "    if masks is not None and len(masks) > 0:\n",
    "        for mask in masks:\n",
    "            mask_resized = cv2.resize(mask[0].astype(np.uint8), (gt_mask.shape[1], gt_mask.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "            combined_mask = np.logical_or(combined_mask, mask_resized.astype(bool))\n",
    "    \n",
    "    axes[2].imshow(combined_mask.astype(int), cmap='gray')\n",
    "    axes[2].set_title(f'Predicted Mask (Prompt: {prompt})')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    overlay = image.copy()\n",
    "    if image.shape[:2] != combined_mask.shape:\n",
    "        combined_mask = cv2.resize(combined_mask.astype(np.uint8), (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "    overlay[combined_mask] = overlay[combined_mask] * 0.6 + np.array([1, 0, 0]) * 0.4\n",
    "    axes[3].imshow(overlay)\n",
    "    axes[3].set_title('Overlay')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for idx in test_indices[:1]:\n",
    "    image, img_path = dataset[idx]\n",
    "    gt_mask = load_ground_truth_mask(img_path, data_dir)\n",
    "    \n",
    "    prompt = \"car. vehicle\"\n",
    "    result_image, boxes, masks, phrases = segment_by_text_prompt(\n",
    "        image, \n",
    "        prompt,\n",
    "        box_threshold=0.3,\n",
    "        text_threshold=0.25\n",
    "    )\n",
    "    \n",
    "    if gt_mask is not None:\n",
    "        compare_with_ground_truth(result_image, masks, gt_mask, prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_py10",
   "language": "python",
   "name": "train_py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
