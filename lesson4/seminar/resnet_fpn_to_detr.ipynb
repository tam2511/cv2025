{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet + FPN → DETR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постепенное построение DETR архитектуры через наследование, начиная с базовой ResNet + FPN. Каждый шаг добавляет один ключевой компонент, приближая нас к полноценной реализации.\n",
    "\n",
    "**Основная идея DETR:** вместо anchor-based детекции используем набор learnable queries, каждый из которых ищет один объект через cross-attention к feature map. Это упрощает pipeline и делает детекцию end-to-end differentiable.\n",
    "\n",
    "**Ключевые отличия от ResNet + FPN:**\n",
    "- ResNet + FPN: $f: \\mathbb{R}^{H \\times W \\times 3} \\to \\mathbb{R}^{H' \\times W' \\times C}$ — плотная feature map для anchor-based детекции\n",
    "- DETR: $f: \\mathbb{R}^{H \\times W \\times 3} \\to \\{(c_i, b_i)\\}_{i=1}^N$ — набор из $N$ пар (класс, bbox)\n",
    "\n",
    "Это позволяет избежать NMS и anchor tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models import resnet50\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зачем начинать с ResNet + FPN?\n",
    "\n",
    "ResNet — это мощный backbone для извлечения признаков из изображений. Он использует residual connections, которые позволяют обучать очень глубокие сети.\n",
    "\n",
    "FPN (Feature Pyramid Network) добавляет multi-scale feature extraction через top-down pathway с lateral connections. Это позволяет комбинировать высокоуровневую семантическую информацию с низкоуровневыми деталями.\n",
    "\n",
    "В классической object detection ResNet + FPN используется как backbone для генерации dense predictions с anchors. Но в DETR мы будем использовать только feature extraction часть, а detection head заменим на transformer с queries.\n",
    "\n",
    "Архитектура ResNet + FPN:\n",
    "$$\n",
    "\\text{Image} \\xrightarrow{\\text{ResNet}} \\{C_2, C_3, C_4, C_5\\} \\xrightarrow{\\text{FPN}} \\{P_2, P_3, P_4, P_5\\}\n",
    "$$\n",
    "\n",
    "где $C_i$ — выходы ResNet слоев с stride $2^i$, а $P_i$ — FPN features той же размерности.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Base ResNet + FPN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: FPN outputs with shapes [torch.Size([2, 256, 160, 160]), torch.Size([2, 256, 80, 80]), torch.Size([2, 256, 40, 40]), torch.Size([2, 256, 20, 20])]\n"
     ]
    }
   ],
   "source": [
    "class FPN(nn.Module):\n",
    "    def __init__(self, in_channels_list, out_channels=256):\n",
    "        super().__init__()\n",
    "        self.lateral_convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, out_channels, 1) for in_channels in in_channels_list\n",
    "        ])\n",
    "        self.fpn_convs = nn.ModuleList([\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1) for _ in in_channels_list\n",
    "        ])\n",
    "    \n",
    "    def forward(self, features):\n",
    "        laterals = [lateral_conv(features[i]) for i, lateral_conv in enumerate(self.lateral_convs)]\n",
    "        \n",
    "        for i in range(len(laterals) - 1, 0, -1):\n",
    "            laterals[i - 1] += F.interpolate(laterals[i], size=laterals[i - 1].shape[-2:], mode='nearest')\n",
    "        \n",
    "        fpn_features = [fpn_conv(lateral) for fpn_conv, lateral in zip(self.fpn_convs, laterals)]\n",
    "        return fpn_features\n",
    "\n",
    "class ResNetFPN(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super().__init__()\n",
    "        backbone = resnet50(pretrained=pretrained)\n",
    "        self.conv1 = backbone.conv1\n",
    "        self.bn1 = backbone.bn1\n",
    "        self.relu = backbone.relu\n",
    "        self.maxpool = backbone.maxpool\n",
    "        \n",
    "        self.layer1 = backbone.layer1\n",
    "        self.layer2 = backbone.layer2\n",
    "        self.layer3 = backbone.layer3\n",
    "        self.layer4 = backbone.layer4\n",
    "        \n",
    "        self.fpn = FPN([256, 512, 1024, 2048], out_channels=256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        c1 = self.layer1(x)\n",
    "        c2 = self.layer2(c1)\n",
    "        c3 = self.layer3(c2)\n",
    "        c4 = self.layer4(c3)\n",
    "        \n",
    "        features = self.fpn([c1, c2, c3, c4])\n",
    "        return features\n",
    "\n",
    "class Step0(ResNetFPN):\n",
    "    pass\n",
    "\n",
    "model = Step0(pretrained=False)\n",
    "features = model(torch.randn(2, 3, 640, 640))\n",
    "print(f\"Step 0: FPN outputs with shapes {[f.shape for f in features]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зачем нужна единая feature map?\n",
    "\n",
    "В DETR мы не используем multi-scale features напрямую, как в классических детекторах. Вместо этого берём feature map с одного уровня и работаем с ней через transformer.\n",
    "\n",
    "Обычно используют выход последнего слоя FPN (или даже напрямую выход ResNet без FPN). Для простоты возьмём последний уровень FPN: $P_5 \\in \\mathbb{R}^{B \\times 256 \\times H' \\times W'}$, где $H' = H/32$, $W' = W/32$.\n",
    "\n",
    "Эта feature map будет использоваться как \"память\" для transformer decoder — queries будут смотреть на неё через cross-attention, чтобы найти объекты.\n",
    "\n",
    "В более продвинутых версиях DETR (Deformable DETR) используются multi-scale features с deformable attention, но для базовой версии достаточно одного уровня.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract Single Feature Map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Single feature map torch.Size([2, 256, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "class Step1(ResNetFPN):\n",
    "    def __init__(self, emb_dim=256):\n",
    "        super().__init__(pretrained=False)\n",
    "        self.emb_dim = emb_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = super().forward(x)\n",
    "        return features[-1]\n",
    "\n",
    "model = Step1(emb_dim=256)\n",
    "feature_map = model(torch.randn(2, 3, 640, 640))\n",
    "print(f\"Step 1: Single feature map {feature_map.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переход к query-based подходу\n",
    "\n",
    "Ключевая идея DETR: вместо плотных predictions на каждом пикселе или для каждого anchor, мы создаём фиксированное число learnable queries $Q \\in \\mathbb{R}^{N \\times d}$, где $N=100$ — максимальное количество объектов, которое мы хотим детектировать на одном изображении.\n",
    "\n",
    "Каждый query — это learned embedding, который будет \"искать\" один объект. На выходе каждый query предсказывает:\n",
    "- Класс объекта: $p_i \\in \\mathbb{R}^{C+1}$ (дополнительный класс \"no object\")\n",
    "- Bounding box: $b_i \\in \\mathbb{R}^4$ в формате $(c_x, c_y, w, h)$ normalized\n",
    "\n",
    "Пока просто пропускаем queries через линейные слои:\n",
    "$$\n",
    "\\text{class\\_logits} = \\text{Linear}(Q) \\in \\mathbb{R}^{B \\times N \\times (C+1)}\n",
    "$$\n",
    "$$\n",
    "\\text{bbox\\_pred} = \\text{Sigmoid}(\\text{Linear}(Q)) \\in \\mathbb{R}^{B \\times N \\times 4}\n",
    "$$\n",
    "\n",
    "Queries пока статичные, но скоро добавим transformer для их обогащения информацией из изображения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: class_logits=torch.Size([2, 100, 81]), bbox_pred=torch.Size([2, 100, 4])\n"
     ]
    }
   ],
   "source": [
    "class Step2(Step1):\n",
    "    def __init__(self, num_classes=80, emb_dim=256, num_queries=100):\n",
    "        super().__init__(emb_dim)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_queries = num_queries\n",
    "        self.queries = nn.Embedding(num_queries, emb_dim)\n",
    "        self.class_head = nn.Linear(emb_dim, num_classes + 1)\n",
    "        self.bbox_head = nn.Linear(emb_dim, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        features = super().forward(x)\n",
    "        \n",
    "        queries = self.queries.weight.unsqueeze(0).expand(B, -1, -1)\n",
    "        class_logits = self.class_head(queries)\n",
    "        bbox_pred = self.bbox_head(queries).sigmoid()\n",
    "        \n",
    "        return {'class_logits': class_logits, 'bbox_pred': bbox_pred, 'features': features}\n",
    "\n",
    "model = Step2(num_classes=80)\n",
    "out = model(torch.randn(2, 3, 640, 640))\n",
    "print(f\"Step 2: class_logits={out['class_logits'].shape}, bbox_pred={out['bbox_pred'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Почему transformer нужны позиционные эмбеддинги?\n",
    "\n",
    "Transformer изначально придуман для последовательностей, где нет пространственной структуры. Attention механизм сам по себе permutation-invariant — он не знает, где находится каждый элемент.\n",
    "\n",
    "Для изображений это проблема: пиксель в левом верхнем углу должен иметь другое представление, чем пиксель справа внизу.\n",
    "\n",
    "Решение — добавить 2D sinusoidal positional encoding:\n",
    "$$\n",
    "\\text{PE}(x, y, 2i) = \\sin\\left(\\frac{x}{10000^{2i/d}}\\right), \\quad \\text{PE}(x, y, 2i+1) = \\cos\\left(\\frac{x}{10000^{2i/d}}\\right)\n",
    "$$\n",
    "\n",
    "Делаем это отдельно для координат $x$ и $y$, потом конкатенируем. Получаем $\\text{pos\\_emb} \\in \\mathbb{R}^{B \\times d \\times H' \\times W'}$.\n",
    "\n",
    "Также создаём learnable позиционные embeddings для queries — каждый query должен знать свою позицию в наборе.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Add Positional Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: positional encoding added, pos_emb=torch.Size([2, 256, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "class PositionEmbedding2D(nn.Module):\n",
    "    def __init__(self, dim=256, temperature=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        y_embed = torch.arange(H, dtype=torch.float32, device=x.device).view(H, 1).repeat(1, W)\n",
    "        x_embed = torch.arange(W, dtype=torch.float32, device=x.device).view(1, W).repeat(H, 1)\n",
    "        \n",
    "        dim_t = torch.arange(self.dim // 4, dtype=torch.float32, device=x.device)\n",
    "        dim_t = self.temperature ** (2 * dim_t / (self.dim // 4))\n",
    "        \n",
    "        pos_x = x_embed[:, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, None] / dim_t\n",
    "        \n",
    "        pos_x = torch.cat([pos_x.sin(), pos_x.cos()], dim=-1)\n",
    "        pos_y = torch.cat([pos_y.sin(), pos_y.cos()], dim=-1)\n",
    "        \n",
    "        pos = torch.cat([pos_y, pos_x], dim=-1).permute(2, 0, 1).unsqueeze(0).expand(B, -1, -1, -1)\n",
    "        return pos\n",
    "\n",
    "class Step3(Step2):\n",
    "    def __init__(self, num_classes=80, emb_dim=256, num_queries=100):\n",
    "        super().__init__(num_classes, emb_dim, num_queries)\n",
    "        self.pos_emb = PositionEmbedding2D(emb_dim)\n",
    "        self.query_pos = nn.Embedding(num_queries, emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = super().forward(x)\n",
    "        features = out['features']\n",
    "        pos_emb = self.pos_emb(features)\n",
    "        \n",
    "        out['pos_emb'] = pos_emb\n",
    "        out['query_pos'] = self.query_pos.weight\n",
    "        return out\n",
    "\n",
    "model = Step3(num_classes=80)\n",
    "out = model(torch.randn(2, 3, 640, 640))\n",
    "print(f\"Step 3: positional encoding added, pos_emb={out['pos_emb'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зачем нужен Transformer Encoder?\n",
    "\n",
    "В оригинальном DETR перед тем, как queries начнут смотреть на feature map через decoder, сама feature map обогащается через Transformer Encoder.\n",
    "\n",
    "Encoder применяет self-attention к feature map, позволяя каждой позиции \"увидеть\" всё изображение и обновить свои признаки с учётом глобального контекста. Это критично для качества детекции.\n",
    "\n",
    "Архитектура:\n",
    "1. Feature map flatten: $F \\in \\mathbb{R}^{B \\times C \\times H \\times W} \\to \\mathbb{R}^{B \\times (H \\cdot W) \\times C}$\n",
    "2. Добавляем позиционные embeddings: $F + \\text{pos\\_emb}$\n",
    "3. Пропускаем через Transformer Encoder:\n",
    "   $$\n",
    "   F_{\\text{enc}} = \\text{TransformerEncoder}(F + \\text{pos\\_emb})\n",
    "   $$\n",
    "\n",
    "Внутри encoder layer происходит:\n",
    "- Self-attention между всеми позициями feature map\n",
    "- Feed-forward network\n",
    "\n",
    "После encoder получаем обогащённую feature map $F_{\\text{enc}}$, которая будет использоваться как memory для decoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add Transformer Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: encoder applied, mem=torch.Size([2, 400, 256]), class_logits=torch.Size([2, 100, 81])\n"
     ]
    }
   ],
   "source": [
    "class Step4(Step3):\n",
    "    def __init__(self, num_classes=80, emb_dim=256, num_queries=100, nhead=8, enc_layers=6):\n",
    "        super().__init__(num_classes, emb_dim, num_queries)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=emb_dim * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=enc_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        features = Step1.forward(self, x)\n",
    "        pos_emb = self.pos_emb(features)\n",
    "        \n",
    "        mem = features.flatten(2).permute(0, 2, 1)\n",
    "        mem_pos = pos_emb.flatten(2).permute(0, 2, 1)\n",
    "        \n",
    "        mem = self.transformer_encoder(mem + mem_pos)\n",
    "        \n",
    "        queries = self.queries.weight.unsqueeze(0).expand(B, -1, -1)\n",
    "        class_logits = self.class_head(queries)\n",
    "        bbox_pred = self.bbox_head(queries).sigmoid()\n",
    "        \n",
    "        return {'class_logits': class_logits, 'bbox_pred': bbox_pred, 'mem': mem, 'mem_pos': mem_pos}\n",
    "\n",
    "model = Step4(num_classes=80, enc_layers=6)\n",
    "out = model(torch.randn(2, 3, 640, 640))\n",
    "print(f\"Step 4: encoder applied, mem={out['mem'].shape}, class_logits={out['class_logits'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавляем Transformer Decoder\n",
    "\n",
    "Теперь queries могут смотреть на обогащённую encoder'ом feature map через cross-attention. Используем стандартный TransformerDecoder из PyTorch.\n",
    "\n",
    "Как это работает:\n",
    "1. Берём encoded memory $\\text{mem}$ из предыдущего шага\n",
    "2. Queries получают свои позиции: $Q + Q_{\\text{pos}}$\n",
    "3. Пропускаем через Transformer Decoder:\n",
    "   $$\n",
    "   Q' = \\text{TransformerDecoder}(Q + Q_{\\text{pos}}, \\text{mem} + \\text{mem\\_pos})\n",
    "   $$\n",
    "\n",
    "Внутри каждого decoder layer происходит:\n",
    "- Self-attention между queries — queries обмениваются информацией друг с другом\n",
    "- Cross-attention queries → memory — queries смотрят на encoded feature map и ищут объекты\n",
    "- Feed-forward network — нелинейная трансформация\n",
    "\n",
    "После этого queries становятся content-aware — каждый query научился фокусироваться на своём объекте.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Add Transformer Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: decoder applied, class_logits=torch.Size([2, 100, 81]), bbox_pred=torch.Size([2, 100, 4])\n"
     ]
    }
   ],
   "source": [
    "class Step5(Step4):\n",
    "    def __init__(self, num_classes=80, emb_dim=256, num_queries=100, nhead=8, enc_layers=6, dec_layers=6):\n",
    "        super().__init__(num_classes, emb_dim, num_queries, nhead, enc_layers)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=emb_dim * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=dec_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        out = super().forward(x)\n",
    "        \n",
    "        mem = out['mem']\n",
    "        mem_pos = out['mem_pos']\n",
    "        \n",
    "        queries = self.queries.weight.unsqueeze(0).expand(B, -1, -1)\n",
    "        q_pos = self.query_pos.weight.unsqueeze(0).expand(B, -1, -1)\n",
    "        \n",
    "        queries = self.transformer_decoder(queries + q_pos, mem + mem_pos)\n",
    "        \n",
    "        class_logits = self.class_head(queries)\n",
    "        bbox_pred = self.bbox_head(queries).sigmoid()\n",
    "        \n",
    "        return {'class_logits': class_logits, 'bbox_pred': bbox_pred, 'mem': mem, 'mem_pos': mem_pos}\n",
    "\n",
    "model = Step5(num_classes=80, enc_layers=6, dec_layers=6)\n",
    "out = model(torch.randn(2, 3, 640, 640))\n",
    "print(f\"Step 5: decoder applied, class_logits={out['class_logits'].shape}, bbox_pred={out['bbox_pred'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Supervision для ускорения обучения\n",
    "\n",
    "Deep supervision — это техника, когда мы считаем loss не только на финальном выходе, но и на промежуточных слоях transformer. Зачем?\n",
    "\n",
    "1. Помогает градиентам лучше проходить через глубокую сеть\n",
    "2. Заставляет ранние слои transformer сразу учиться предсказывать разумные bbox и классы\n",
    "3. Даёт регуляризацию — модель не может полагаться только на последний слой\n",
    "\n",
    "Модифицируем TransformerDecoder, чтобы он возвращал выходы всех промежуточных слоёв:\n",
    "$$\n",
    "[Q^{(1)}, Q^{(2)}, \\ldots, Q^{(L)}]\n",
    "$$\n",
    "\n",
    "Для каждого $Q^{(l)}$ предсказываем классы и bbox, и суммируем все losses:\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{l=1}^{L} \\mathcal{L}(Q^{(l)}, \\text{targets})\n",
    "$$\n",
    "\n",
    "Финальный выход — это всё равно последний слой $Q^{(L)}$, но обучение идёт быстрее и стабильнее.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deep Supervision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: 5 aux outputs\n",
      "Output: class_logits=torch.Size([2, 100, 81]), bbox_pred=torch.Size([2, 100, 4])\n"
     ]
    }
   ],
   "source": [
    "class TransformerDecoderWithIntermediates(nn.TransformerDecoder):\n",
    "    def forward(self, tgt, memory, return_intermediate=False):\n",
    "        output = tgt\n",
    "        intermediates = []\n",
    "        \n",
    "        for mod in self.layers:\n",
    "            output = mod(output, memory)\n",
    "            if return_intermediate:\n",
    "                intermediates.append(output)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        \n",
    "        return torch.stack(intermediates) if return_intermediate else output\n",
    "\n",
    "class Step6(Step5):\n",
    "    def __init__(self, num_classes=80, emb_dim=256, num_queries=100, nhead=8, enc_layers=6, dec_layers=6):\n",
    "        super().__init__(num_classes, emb_dim, num_queries, nhead, enc_layers, dec_layers)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=emb_dim * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = TransformerDecoderWithIntermediates(decoder_layer, num_layers=dec_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        features = Step1.forward(self, x)\n",
    "        pos_emb = self.pos_emb(features)\n",
    "        \n",
    "        mem = features.flatten(2).permute(0, 2, 1)\n",
    "        mem_pos = pos_emb.flatten(2).permute(0, 2, 1)\n",
    "        \n",
    "        mem = self.transformer_encoder(mem + mem_pos)\n",
    "        \n",
    "        queries = self.queries.weight.unsqueeze(0).expand(B, -1, -1)\n",
    "        q_pos = self.query_pos.weight.unsqueeze(0).expand(B, -1, -1)\n",
    "        \n",
    "        intermediate_queries = self.transformer_decoder(queries + q_pos, mem + mem_pos, return_intermediate=True)\n",
    "        \n",
    "        outputs = []\n",
    "        for layer_queries in intermediate_queries:\n",
    "            class_logits = self.class_head(layer_queries)\n",
    "            bbox_pred = self.bbox_head(layer_queries).sigmoid()\n",
    "            outputs.append({'class_logits': class_logits, 'bbox_pred': bbox_pred})\n",
    "        \n",
    "        final = outputs[-1]\n",
    "        final['aux_outputs'] = outputs[:-1]\n",
    "        return final\n",
    "\n",
    "model = Step6(num_classes=80, enc_layers=6, dec_layers=6)\n",
    "out = model(torch.randn(2, 3, 640, 640))\n",
    "print(f\"Step 6: {len(out['aux_outputs'])} aux outputs\")\n",
    "print(f\"Output: class_logits={out['class_logits'].shape}, bbox_pred={out['bbox_pred'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hungarian Matching: как сопоставить предсказания с ground truth?\n",
    "\n",
    "У нас есть $N=100$ queries, но в изображении только несколько объектов. Какой query должен предсказывать какой объект? Это проблема bipartite matching.\n",
    "\n",
    "Решение: Hungarian algorithm. Идея:\n",
    "1. Строим cost matrix $C \\in \\mathbb{R}^{N \\times M}$, где $M$ — число GT объектов\n",
    "2. $C_{ij}$ — стоимость назначения query $i$ на GT объект $j$\n",
    "3. Ищем оптимальное назначение, минимизирующее суммарную стоимость\n",
    "\n",
    "Cost состоит из трёх компонентов:\n",
    "$$\n",
    "C_{ij} = \\lambda_{\\text{cls}} \\cdot \\mathcal{L}_{\\text{cls}}(p_i, y_j) + \\lambda_{\\text{bbox}} \\cdot \\mathcal{L}_{\\text{L1}}(b_i, \\hat{b}_j) + \\lambda_{\\text{giou}} \\cdot \\mathcal{L}_{\\text{GIoU}}(b_i, \\hat{b}_j)\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $\\mathcal{L}_{\\text{cls}}$ — classification cost (negative probability правильного класса)\n",
    "- $\\mathcal{L}_{\\text{L1}}$ — L1 distance между предсказанным bbox и GT bbox\n",
    "- $\\mathcal{L}_{\\text{GIoU}}$ — Generalized IoU loss\n",
    "\n",
    "После matching знаем, какие queries matched, остальные должны предсказывать \"no object\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Add Hungarian Matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: Matched [3, 2] queries per image\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def box_cxcywh_to_xyxy(boxes):\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "    return torch.stack([x1, y1, x2, y2], dim=-1)\n",
    "\n",
    "def box_iou(boxes1, boxes2):\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    \n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "    \n",
    "    wh = (rb - lt).clamp(min=0)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]\n",
    "    \n",
    "    union = area1[:, None] + area2 - inter\n",
    "    iou = inter / union\n",
    "    return iou\n",
    "\n",
    "def generalized_box_iou(boxes1, boxes2):\n",
    "    iou = box_iou(boxes1, boxes2)\n",
    "    \n",
    "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "    \n",
    "    wh = (rb - lt).clamp(min=0)\n",
    "    area = wh[:, :, 0] * wh[:, :, 1]\n",
    "    \n",
    "    return iou - (area - (box_iou(boxes1, boxes2) * area)) / area\n",
    "\n",
    "class Step7(Step6):\n",
    "    def __init__(self, num_classes=80, emb_dim=256, num_queries=100, nhead=8, enc_layers=6, dec_layers=6):\n",
    "        super().__init__(num_classes, emb_dim, num_queries, nhead, enc_layers, dec_layers)\n",
    "        self.cost_class = 2.0\n",
    "        self.cost_bbox = 5.0\n",
    "        self.cost_giou = 2.0\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def match(self, outputs, targets):\n",
    "        B, Q = outputs['class_logits'].shape[:2]\n",
    "        class_probs = F.softmax(outputs['class_logits'], dim=-1)\n",
    "        bbox_pred = outputs['bbox_pred']\n",
    "        \n",
    "        indices = []\n",
    "        for b in range(B):\n",
    "            if len(targets[b]['labels']) == 0:\n",
    "                indices.append((torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)))\n",
    "                continue\n",
    "            \n",
    "            gt_labels = targets[b]['labels']\n",
    "            gt_boxes = targets[b]['boxes']\n",
    "            \n",
    "            cost_class = -class_probs[b][:, gt_labels]\n",
    "            cost_bbox = torch.cdist(bbox_pred[b], gt_boxes, p=1)\n",
    "            \n",
    "            pred_boxes_xyxy = box_cxcywh_to_xyxy(bbox_pred[b])\n",
    "            gt_boxes_xyxy = box_cxcywh_to_xyxy(gt_boxes)\n",
    "            cost_giou = -generalized_box_iou(pred_boxes_xyxy, gt_boxes_xyxy)\n",
    "            \n",
    "            cost_matrix = self.cost_class * cost_class + self.cost_bbox * cost_bbox + self.cost_giou * cost_giou\n",
    "            pred_idx, gt_idx = linear_sum_assignment(cost_matrix.cpu().numpy())\n",
    "            indices.append((torch.tensor(pred_idx, dtype=torch.long), torch.tensor(gt_idx, dtype=torch.long)))\n",
    "        \n",
    "        return indices\n",
    "\n",
    "model = Step7(num_classes=80, enc_layers=6, dec_layers=6)\n",
    "outputs = model(torch.randn(2, 3, 640, 640))\n",
    "targets = [\n",
    "    {'labels': torch.tensor([1, 2, 5]), 'boxes': torch.rand(3, 4)},\n",
    "    {'labels': torch.tensor([0, 3]), 'boxes': torch.rand(2, 4)}\n",
    "]\n",
    "matches = model.match(outputs, targets)\n",
    "print(f\"Step 7: Matched {[len(m[0]) for m in matches]} queries per image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DETR Loss: комбинация трёх компонентов\n",
    "\n",
    "После matching можем посчитать loss. Используем три компонента:\n",
    "\n",
    "**1. Classification Loss** — cross-entropy для всех $N$ queries:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{cls}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_i(y_i)\n",
    "$$\n",
    "где $y_i$ — matched класс (или \"no object\" для unmatched queries).\n",
    "\n",
    "**2. L1 Loss** — только для matched queries:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{L1}} = \\frac{1}{M} \\sum_{i \\in \\text{matched}} \\|b_i - \\hat{b}_i\\|_1\n",
    "$$\n",
    "L1 loss измеряет абсолютное отклонение координат bbox.\n",
    "\n",
    "**3. GIoU Loss** — только для matched queries:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{GIoU}} = \\frac{1}{M} \\sum_{i \\in \\text{matched}} (1 - \\text{GIoU}(b_i, \\hat{b}_i))\n",
    "$$\n",
    "GIoU (Generalized IoU) — это улучшенная версия IoU, которая учитывает не только пересечение, но и форму описывающего прямоугольника.\n",
    "\n",
    "Итоговый loss:\n",
    "$$\n",
    "\\mathcal{L} = \\lambda_{\\text{cls}} \\mathcal{L}_{\\text{cls}} + \\lambda_{\\text{L1}} \\mathcal{L}_{\\text{L1}} + \\lambda_{\\text{GIoU}} \\mathcal{L}_{\\text{GIoU}}\n",
    "$$\n",
    "\n",
    "С deep supervision суммируем losses со всех слоёв.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Add Loss Computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: Loss = 94.2215\n"
     ]
    }
   ],
   "source": [
    "def giou_loss(boxes1, boxes2):\n",
    "    boxes1_xyxy = box_cxcywh_to_xyxy(boxes1)\n",
    "    boxes2_xyxy = box_cxcywh_to_xyxy(boxes2)\n",
    "    \n",
    "    giou = torch.diag(generalized_box_iou(boxes1_xyxy, boxes2_xyxy))\n",
    "    return (1 - giou).mean()\n",
    "\n",
    "class Step8(Step7):\n",
    "    def __init__(self, num_classes=80, emb_dim=256, num_queries=100, nhead=8, enc_layers=6, dec_layers=6, w_class=2.0, w_bbox=5.0, w_giou=2.0):\n",
    "        super().__init__(num_classes, emb_dim, num_queries, nhead, enc_layers, dec_layers)\n",
    "        self.w_class = w_class\n",
    "        self.w_bbox = w_bbox\n",
    "        self.w_giou = w_giou\n",
    "    \n",
    "    def compute_loss(self, outputs, targets):\n",
    "        indices = self.match(outputs, targets)\n",
    "        loss = self._compute_single_loss(outputs, targets, indices)\n",
    "        \n",
    "        if 'aux_outputs' in outputs:\n",
    "            for aux_out in outputs['aux_outputs']:\n",
    "                aux_indices = self.match(aux_out, targets)\n",
    "                loss += self._compute_single_loss(aux_out, targets, aux_indices)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _compute_single_loss(self, outputs, targets, indices):\n",
    "        B, Q = outputs['class_logits'].shape[:2]\n",
    "        target_classes = torch.full((B, Q), self.num_classes, dtype=torch.long, device=outputs['class_logits'].device)\n",
    "        \n",
    "        for b, (pred_idx, gt_idx) in enumerate(indices):\n",
    "            if len(pred_idx) > 0:\n",
    "                target_classes[b, pred_idx] = targets[b]['labels'][gt_idx]\n",
    "        \n",
    "        loss_class = F.cross_entropy(outputs['class_logits'].flatten(0, 1), target_classes.flatten())\n",
    "        \n",
    "        loss_bbox, loss_giou, num_boxes = 0, 0, 0\n",
    "        for b, (pred_idx, gt_idx) in enumerate(indices):\n",
    "            if len(pred_idx) == 0:\n",
    "                continue\n",
    "            pred_boxes = outputs['bbox_pred'][b, pred_idx]\n",
    "            gt_boxes = targets[b]['boxes'][gt_idx]\n",
    "            loss_bbox += F.l1_loss(pred_boxes, gt_boxes, reduction='sum')\n",
    "            loss_giou += giou_loss(pred_boxes, gt_boxes) * len(pred_idx)\n",
    "            num_boxes += len(pred_idx)\n",
    "        \n",
    "        if num_boxes > 0:\n",
    "            loss_bbox /= num_boxes\n",
    "            loss_giou /= num_boxes\n",
    "        \n",
    "        return self.w_class * loss_class + self.w_bbox * loss_bbox + self.w_giou * loss_giou\n",
    "\n",
    "model = Step8(num_classes=80, enc_layers=6, dec_layers=6)\n",
    "outputs = model(torch.randn(2, 3, 640, 640))\n",
    "targets = [\n",
    "    {'labels': torch.tensor([1, 2, 5]), 'boxes': torch.rand(3, 4)},\n",
    "    {'labels': torch.tensor([0, 3]), 'boxes': torch.rand(2, 4)}\n",
    "]\n",
    "loss = model.compute_loss(outputs, targets)\n",
    "print(f\"Step 8: Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done\n",
    "\n",
    "Мы построили полноценную архитектуру DETR из классического ResNet + FPN backbone. Текущая реализация включает все ключевые компоненты оригинального DETR:\n",
    "\n",
    "1. **Backbone** — ResNet + FPN для извлечения признаков\n",
    "2. **Queries** — learnable embeddings для поиска объектов\n",
    "3. **Positional Encoding** — 2D sinusoidal позиции для feature map\n",
    "4. **Transformer Encoder** — self-attention для обогащения feature map глобальным контекстом\n",
    "5. **Transformer Decoder** — cross-attention между queries и encoded features\n",
    "6. **Detection Heads** — предсказание классов и bounding boxes для каждого query\n",
    "7. **Deep Supervision** — auxiliary losses для всех decoder layers\n",
    "8. **Hungarian Matching** — оптимальное сопоставление предсказаний с GT через bipartite matching\n",
    "9. **Combined Loss** — classification + L1 + GIoU loss\n",
    "\n",
    "Основные преимущества DETR перед классическими детекторами:\n",
    "- Не нужны anchors и их настройка\n",
    "- Не нужен NMS для фильтрации дубликатов\n",
    "- End-to-end differentiable pipeline\n",
    "- и тд\n",
    "\n",
    "Недостатки:\n",
    "- Медленная сходимость (требует много эпох обучения)\n",
    "- Плохо работает с маленькими объектами\n",
    "- и тд\n",
    "\n",
    "Эти проблемы решаются в более продвинутых версиях: Deformable DETR, Conditional DETR, DINO.\n",
    "\n",
    "В целом по сравнению с mask2former тут мы реализовали detr полноценно :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_py10",
   "language": "python",
   "name": "train_py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
