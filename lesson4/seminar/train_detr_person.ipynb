{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from config import load_config\n",
    "from dataset_coco import COCODetectionDataset\n",
    "from models import DETR\n",
    "from lightning_module import DetectionModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('./configs/config_coco_detr_person.json')\n",
    "\n",
    "DATA_ROOT = config['data']['root_dir']\n",
    "TRAIN_SPLIT = config['data']['split']\n",
    "VAL_SPLIT = config['data']['val_split']\n",
    "TEST_SPLIT = config['data']['test_split']\n",
    "TARGET_SIZE = tuple(config['data']['target_size'])\n",
    "BATCH_SIZE = config['data']['batch_size']\n",
    "NUM_WORKERS = config['data']['num_workers']\n",
    "MIN_AREA = config['data']['min_area']\n",
    "FILTER_CATEGORIES = config['data']['filter_categories']\n",
    "NUM_CLASSES = config['model']['num_classes']\n",
    "\n",
    "MAX_EPOCHS = config['training']['max_epochs']\n",
    "LEARNING_RATE = config['training']['learning_rate']\n",
    "WEIGHT_DECAY = config['training']['weight_decay']\n",
    "OPTIMIZER = config['training']['optimizer']\n",
    "\n",
    "NUM_QUERIES = config['model']['num_queries']\n",
    "EMB_DIM = config['model']['emb_dim']\n",
    "NHEAD = config['model']['nhead']\n",
    "ENC_LAYERS = config['model']['enc_layers']\n",
    "DEC_LAYERS = config['model']['dec_layers']\n",
    "PRETRAINED = config['model']['pretrained']\n",
    "\n",
    "print(f\"Model: DETR\")\n",
    "print(f\"Classes: {NUM_CLASSES} ({', '.join(FILTER_CATEGORIES)})\")\n",
    "print(f\"Num queries: {NUM_QUERIES}\")\n",
    "print(f\"Encoder layers: {ENC_LAYERS}, Decoder layers: {DEC_LAYERS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    boxes = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "    return images, boxes, labels\n",
    "\n",
    "train_dataset = COCODetectionDataset(\n",
    "    root_dir=DATA_ROOT,\n",
    "    split=TRAIN_SPLIT,\n",
    "    target_size=TARGET_SIZE,\n",
    "    min_area=MIN_AREA,\n",
    "    filter_categories=FILTER_CATEGORIES\n",
    ")\n",
    "\n",
    "val_dataset = COCODetectionDataset(\n",
    "    root_dir=DATA_ROOT,\n",
    "    split=VAL_SPLIT,\n",
    "    target_size=TARGET_SIZE,\n",
    "    min_area=MIN_AREA,\n",
    "    filter_categories=FILTER_CATEGORIES\n",
    ")\n",
    "\n",
    "test_dataset = COCODetectionDataset(\n",
    "    root_dir=DATA_ROOT,\n",
    "    split=TEST_SPLIT,\n",
    "    target_size=TARGET_SIZE,\n",
    "    min_area=MIN_AREA,\n",
    "    filter_categories=FILTER_CATEGORIES\n",
    ")\n",
    "\n",
    "print(f'Train samples: {len(train_dataset)}')\n",
    "print(f'Val samples: {len(val_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f'Train batches: {len(train_loader)}')\n",
    "print(f'Val batches: {len(val_loader)}')\n",
    "print(f'Test batches: {len(test_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DETR(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    emb_dim=EMB_DIM,\n",
    "    num_queries=NUM_QUERIES,\n",
    "    nhead=NHEAD,\n",
    "    enc_layers=ENC_LAYERS,\n",
    "    dec_layers=DEC_LAYERS,\n",
    "    pretrained=PRETRAINED\n",
    ")\n",
    "\n",
    "lightning_module = DetectionModule(\n",
    "    model=model,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    optimizer=OPTIMIZER\n",
    ")\n",
    "\n",
    "print(f\"Model initialized\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=config['training']['checkpoint_dirpath'],\n",
    "    filename=config['training']['checkpoint_filename'],\n",
    "    monitor=config['training']['checkpoint_monitor'],\n",
    "    mode=config['training']['checkpoint_mode'],\n",
    "    save_top_k=config['training']['checkpoint_save_top_k'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=config['training']['checkpoint_monitor'],\n",
    "    patience=config['training']['early_stopping_patience'],\n",
    "    mode=config['training']['checkpoint_mode'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=config['training']['log_dir'],\n",
    "    name=config['training']['experiment_name']\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    accelerator=config['hardware']['accelerator'],\n",
    "    devices=config['hardware']['devices'],\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=config['training']['log_every_n_steps'],\n",
    "    deterministic=False\n",
    ")\n",
    "\n",
    "print(\"Trainer configured\")\n",
    "print(f\"Max epochs: {MAX_EPOCHS}\")\n",
    "print(f\"Device: {config['hardware']['accelerator']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(lightning_module, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best model path: {best_model_path}\")\n",
    "\n",
    "trainer.test(lightning_module, test_loader, ckpt_path=best_model_path if best_model_path else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "images_vis, boxes_vis, labels_vis = next(iter(test_loader))\n",
    "images_vis = images_vis[:4].to(lightning_module.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = lightning_module(images_vis)\n",
    "    preds = lightning_module.model.postprocess(outputs, conf_threshold=0.8)\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(4):\n",
    "    img = images_vis[i].cpu().numpy().transpose(1, 2, 0)\n",
    "    img = img * std + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    \n",
    "    for j in range(len(boxes_vis[i])):\n",
    "        cx, cy, w, h = boxes_vis[i][j].numpy()\n",
    "        x1 = (cx - w / 2) * TARGET_SIZE[1]\n",
    "        y1 = (cy - h / 2) * TARGET_SIZE[0]\n",
    "        w_px = w * TARGET_SIZE[1]\n",
    "        h_px = h * TARGET_SIZE[0]\n",
    "        \n",
    "        rect = patches.Rectangle((x1, y1), w_px, h_px, linewidth=2, edgecolor='lime', facecolor='none', linestyle='--')\n",
    "        axes[i].add_patch(rect)\n",
    "        axes[i].text(x1, y1-5, 'GT', color='lime', fontsize=10, weight='bold',\n",
    "                     bbox=dict(facecolor='black', alpha=0.7))\n",
    "    \n",
    "    pred_boxes = preds[i]['boxes']\n",
    "    pred_scores = preds[i]['scores']\n",
    "    for j in range(len(pred_boxes)):\n",
    "        cx, cy, w, h = pred_boxes[j].cpu().numpy()\n",
    "        score = pred_scores[j].cpu().item()\n",
    "        x1 = (cx - w / 2) * TARGET_SIZE[1]\n",
    "        y1 = (cy - h / 2) * TARGET_SIZE[0]\n",
    "        w_px = w * TARGET_SIZE[1]\n",
    "        h_px = h * TARGET_SIZE[0]\n",
    "        \n",
    "        color = 'red' if score < 0.5 else 'yellow' if score < 0.7 else 'cyan'\n",
    "        rect = patches.Rectangle((x1, y1), w_px, h_px, linewidth=2, edgecolor=color, facecolor='none')\n",
    "        axes[i].add_patch(rect)\n",
    "        axes[i].text(x1+w_px, y1, f'{score:.2f}', color=color, fontsize=8, weight='bold',\n",
    "                     bbox=dict(facecolor='black', alpha=0.7))\n",
    "    \n",
    "    axes[i].set_title(f'Image {i+1}: GT={len(boxes_vis[i])} (green dashed), Pred={len(pred_boxes)} (colored)')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Detection Results: GT (green dashed) vs Predictions (colored by confidence)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDetection summary:\")\n",
    "for i in range(4):\n",
    "    print(f\"Image {i+1}: GT={len(boxes_vis[i])}, Predictions={len(preds[i]['boxes'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Position Embedding: Spatial Patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "images_int, _, _ = next(iter(test_loader))\n",
    "images_int = images_int[:1].to(lightning_module.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = lightning_module.model.backbone(images_int)\n",
    "    pos_embed = lightning_module.model.pos_emb(features)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i in range(8):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    ax.imshow(pos_embed[0, i * 32].cpu(), cmap='RdBu')\n",
    "    ax.set_title(f'Channel {i*32}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Position Embedding Channels (sin/cos patterns)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Position embedding shape: {pos_embed.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Query Attention: What Each Query Sees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = lightning_module(images_int)\n",
    "    \n",
    "    mem = features.flatten(2).permute(0, 2, 1)\n",
    "    mem_pos = pos_embed.flatten(2).permute(0, 2, 1)\n",
    "    \n",
    "    mem_encoded = lightning_module.model.transformer_encoder(mem + mem_pos)\n",
    "    \n",
    "    queries = lightning_module.model.queries.weight.unsqueeze(0).expand(1, -1, -1)\n",
    "    q_pos = lightning_module.model.query_pos.weight.unsqueeze(0).expand(1, -1, -1)\n",
    "    \n",
    "    Q = queries + q_pos\n",
    "    K = mem_encoded + mem_pos\n",
    "    attention = torch.matmul(Q, K.transpose(-2, -1)) / (Q.shape[-1] ** 0.5)\n",
    "    attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "H, W = features.shape[2:]\n",
    "attention_maps = attention[0].view(100, H, W)\n",
    "\n",
    "class_probs = torch.softmax(outputs['class_logits'][0], dim=-1)\n",
    "person_probs = class_probs[:, 0]\n",
    "top_queries = torch.argsort(person_probs, descending=True)[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for idx, q in enumerate(top_queries):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    attn_map = attention_maps[q].cpu()\n",
    "    attn_map = F.interpolate(attn_map.unsqueeze(0).unsqueeze(0), \n",
    "                             size=(TARGET_SIZE[0], TARGET_SIZE[1]), mode='bilinear')[0, 0]\n",
    "    ax.imshow(attn_map, cmap='hot')\n",
    "    ax.set_title(f'Query {q.item()}, confidence={person_probs[q]:.3f}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Query-to-Pixel Attention Maps (top 6 confident queries)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. BBox Predictions: Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "bbox_preds = outputs['bbox_pred'][0]\n",
    "\n",
    "img = images_int[0].cpu().numpy().transpose(1, 2, 0)\n",
    "img = img * std + mean\n",
    "img = np.clip(img, 0, 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for idx, q in enumerate(top_queries):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    cx, cy, w, h = bbox_preds[q].cpu().numpy()\n",
    "    x1 = (cx - w / 2) * TARGET_SIZE[1]\n",
    "    y1 = (cy - h / 2) * TARGET_SIZE[0]\n",
    "    w_px = w * TARGET_SIZE[1]\n",
    "    h_px = h * TARGET_SIZE[0]\n",
    "    \n",
    "    rect = patches.Rectangle((x1, y1), w_px, h_px, linewidth=3, edgecolor='red', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.set_title(f'Query {q.item()}, conf={person_probs[q]:.3f}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('BBox Predictions (top 6 confident queries)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'BBox predictions shape: {bbox_preds.shape}')\n",
    "print(f'BBox coords range: [{bbox_preds.min():.3f}, {bbox_preds.max():.3f}]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hungarian Matching: Cost Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "test_img, test_boxes, test_labels = next(iter(test_loader))\n",
    "test_img = test_img[:1].to(lightning_module.device)\n",
    "\n",
    "targets = [{\n",
    "    'boxes': test_boxes[0].to(lightning_module.device),\n",
    "    'labels': test_labels[0].to(lightning_module.device)\n",
    "}]\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = lightning_module(test_img)\n",
    "    \n",
    "    class_probs = torch.softmax(test_outputs['class_logits'][0], dim=-1)\n",
    "    bbox_pred = test_outputs['bbox_pred'][0]\n",
    "    \n",
    "    gt_labels = targets[0]['labels']\n",
    "    gt_boxes = targets[0]['boxes']\n",
    "    \n",
    "    if len(gt_labels) > 0:\n",
    "        cost_class = -class_probs[:, gt_labels]\n",
    "        cost_bbox = torch.cdist(bbox_pred, gt_boxes, p=1)\n",
    "        \n",
    "        def box_cxcywh_to_xyxy(boxes):\n",
    "            cx, cy, w, h = boxes.unbind(-1)\n",
    "            return torch.stack([cx - 0.5 * w, cy - 0.5 * h, cx + 0.5 * w, cy + 0.5 * h], dim=-1)\n",
    "        \n",
    "        def box_iou(boxes1, boxes2):\n",
    "            area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "            area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "            lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
    "            rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "            wh = (rb - lt).clamp(min=0)\n",
    "            inter = wh[:, :, 0] * wh[:, :, 1]\n",
    "            union = area1[:, None] + area2 - inter\n",
    "            return inter / (union + 1e-6)\n",
    "        \n",
    "        def generalized_box_iou(boxes1, boxes2):\n",
    "            iou = box_iou(boxes1, boxes2)\n",
    "            lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
    "            rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "            wh = (rb - lt).clamp(min=0)\n",
    "            area = wh[:, :, 0] * wh[:, :, 1]\n",
    "            union_area = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "            union_area = union_area[:, None] + (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "            union_area = union_area - iou * union_area\n",
    "            return iou - (area - union_area) / (area + 1e-6)\n",
    "        \n",
    "        pred_boxes_xyxy = box_cxcywh_to_xyxy(bbox_pred)\n",
    "        gt_boxes_xyxy = box_cxcywh_to_xyxy(gt_boxes)\n",
    "        cost_giou = -generalized_box_iou(pred_boxes_xyxy, gt_boxes_xyxy)\n",
    "        \n",
    "        cost_matrix = 2.0 * cost_class + 5.0 * cost_bbox + 2.0 * cost_giou\n",
    "        \n",
    "        pred_idx, gt_idx = linear_sum_assignment(cost_matrix.cpu().numpy())\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "        \n",
    "        im1 = axes[0].imshow(cost_class.cpu(), aspect='auto', cmap='RdYlGn_r')\n",
    "        axes[0].set_title('Cost: Class CE')\n",
    "        axes[0].set_xlabel('GT instances')\n",
    "        axes[0].set_ylabel('Queries')\n",
    "        plt.colorbar(im1, ax=axes[0])\n",
    "        \n",
    "        im2 = axes[1].imshow(cost_bbox.cpu(), aspect='auto', cmap='RdYlGn_r')\n",
    "        axes[1].set_title('Cost: L1 BBox')\n",
    "        axes[1].set_xlabel('GT instances')\n",
    "        axes[1].set_ylabel('Queries')\n",
    "        plt.colorbar(im2, ax=axes[1])\n",
    "        \n",
    "        im3 = axes[2].imshow(cost_giou.cpu(), aspect='auto', cmap='RdYlGn_r')\n",
    "        axes[2].set_title('Cost: GIoU')\n",
    "        axes[2].set_xlabel('GT instances')\n",
    "        axes[2].set_ylabel('Queries')\n",
    "        plt.colorbar(im3, ax=axes[2])\n",
    "        \n",
    "        im4 = axes[3].imshow(cost_matrix.cpu(), aspect='auto', cmap='RdYlGn_r')\n",
    "        axes[3].set_title('Total Cost + Matches')\n",
    "        axes[3].set_xlabel('GT instances')\n",
    "        axes[3].set_ylabel('Queries')\n",
    "        for p, g in zip(pred_idx, gt_idx):\n",
    "            axes[3].plot(g, p, 'ro', markersize=10)\n",
    "        plt.colorbar(im4, ax=axes[3])\n",
    "        \n",
    "        plt.suptitle('Hungarian Matching: Cost Matrix Visualization', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f'Cost matrix shape: {cost_matrix.shape}')\n",
    "        print(f'Num matches: {len(pred_idx)}')\n",
    "        print(f'Matched queries: {pred_idx.tolist()}')\n",
    "        print(f'Matched GT indices: {gt_idx.tolist()}')\n",
    "    else:\n",
    "        print('No GT instances in this image')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_py10",
   "language": "python",
   "name": "train_py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
